[Continued from...](ML%2010-03-23%20Lecture%2010.md)

## Module 5: Ensemble Methods

### Gradient Boosting
Idea:
- Instead of updating weights, Gradient Boosting fits the new estimator to the "residual errors."

### Summary - Bagging vs. Boosting
- Both bagging and boosting are ensemble methods aiming to improve "weak" learners.
- Bagging is a parallel method, while boosting is sequential.
- Bagging lets models overfit, while boosting starts with an under-fitted model.
- Bagging handles outliers well, while boosting is prone to outliers.

![center](../zassets/Pasted%20image%2020231005110658.png)


